{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO solution for the Reacher environment from Unity ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def debug(item, name, print_e=False, only_shape=False):\n",
    "    print(\"New item: {}\".format(name))\n",
    "    if only_shape:\n",
    "        print(f\"Shape: {item.shape}\")\n",
    "        return\n",
    "    print(f\"Type: {type(item)}\")\n",
    "    if print_e:\n",
    "        pprint(item)\n",
    "    try:\n",
    "        print(f\"Length: {len(item)}\")\n",
    "        pprint(f\"First element: {item[0]}\")\n",
    "        pprint(f\"Last element: {item[-1]}\")\n",
    "        try:\n",
    "            print(f\"Shape: {item.shape}\")\n",
    "        except:\n",
    "\n",
    "            pass\n",
    "        try:\n",
    "            pprint(\"First element shape: {}\".format(item[0].shape))\n",
    "        except Exception as e:\n",
    "            pprint(e)\n",
    "    except:\n",
    "        print(\"Object has no length\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Agent and models\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Agent hyperparameters\n",
    "BATCH_SIZE = 32         # minibatch size\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TAU = 0.95              # GAE parameter\n",
    "BETA = 0.01             # entropy regularization parameter\n",
    "PPO_CLIP_EPSILON = 0.2  # ppo clip parameter\n",
    "GRADIENT_CLIP = 5       # gradient clipping parameter\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "    \n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, value_size=1, hidden_size=64):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, value_size)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    \n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, value_size=1, hidden_size=64, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = Actor(state_size, action_size, hidden_size)\n",
    "        self.critic = Critic(state_size, value_size, hidden_size)\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_size)*std)\n",
    "        \n",
    "    def forward(self, states): # TODO: LEARN WHAT THE FUCK THIS DOES\n",
    "        obs = torch.FloatTensor(states)\n",
    "        \n",
    "        # Critic\n",
    "        values = self.critic(obs)\n",
    "        \n",
    "        # Actor\n",
    "        mu = self.actor(obs)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        \n",
    "        return dist, values\n",
    "    \n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self, num_agents, state_size, action_size):\n",
    "        self.num_agents = num_agents\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.model = ActorCritic(state_size, action_size, value_size=1)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LR, eps=EPSILON)\n",
    "        self.model.train()\n",
    "        \n",
    "    def act(self, states): # TODO: IS THIS CORRECT? WE SHOULD USE MU AS THE ACTIONS, AND NOT SAMPLE FROM THE DISTRIBUTION\n",
    "        \"\"\"Remember: states are state vectors for each agent\n",
    "        It is used when collecting trajectories\n",
    "        \"\"\"\n",
    "        dist, values = self.model(states) # pass the state trough the network and get a distribution over actions and the value of the state\n",
    "        actions = dist.sample() # sample an action from the distribution\n",
    "        log_probs = dist.log_prob(actions) # calculate the log probability of that action\n",
    "        log_probs = log_probs.sum(-1).unsqueeze(-1) # sum the log probabilities of all actions taken (in case of multiple actions) and reshape to (batch_size, 1)\n",
    "        \n",
    "        return actions, log_probs, values\n",
    "\n",
    "    # def batcher(self, BATCH_SIZE, states, actions, log_probs_old, returns, advantages):\n",
    "    #     \"\"\"Convert trajectories into learning batches.\"\"\"\n",
    "    #     # for _ in range(states.size(0) // BATCH_SIZE):\n",
    "    #     rand_ids = np.random.randint(0, states.size(0), BATCH_SIZE)\n",
    "    #     yield states[rand_ids, :], actions[rand_ids, :], log_probs_old[rand_ids, :], returns[rand_ids, :], advantages[rand_ids, :]\n",
    "\n",
    "    \n",
    "    def learn(self, states, actions, log_probs_old, returns, advantages, sgd_epochs=4):\n",
    "        \"\"\" Performs a learning step given a batch of experiences\n",
    "        \n",
    "        Remmeber: in the PPO algorithm, we perform SGD_episodes (usually 4) weights update steps per batch\n",
    "        using the proximal policy ratio clipped objective function\n",
    "        \"\"\"        \n",
    "\n",
    "        num_batches = states.size(0) // BATCH_SIZE\n",
    "        for i in range(sgd_epochs):\n",
    "            # batch_count = 0\n",
    "            # batch_ind = 0\n",
    "            # for i in range(num_batches):\n",
    "            #     sampled_states = states[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "            #     sampled_actions = actions[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "            #     sampled_log_probs_old = log_probs_old[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "            #     sampled_returns = returns[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "            #     sampled_advantages = advantages[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "                \n",
    "            #     L = ppo_loss(self.model, sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages)\n",
    "                \n",
    "            #     self.optimizer.zero_grad()\n",
    "            #     (L).backward()\n",
    "            #     nn.utils.clip_grad_norm_(self.model.parameters(), GRADIENT_CLIP)\n",
    "            #     self.optimizer.step()\n",
    "                \n",
    "            #     batch_ind += BATCH_SIZE\n",
    "            #     batch_count += 1\n",
    "            \n",
    "            \n",
    "            batch_count = 0\n",
    "            batch_ind = 0\n",
    "            for i in range(num_batches):\n",
    "                sampled_states = states[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "                sampled_actions = actions[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "                sampled_log_probs_old = log_probs_old[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "                sampled_returns = returns[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "                sampled_advantages = advantages[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "                \n",
    "                dist, values = self.model(sampled_states)\n",
    "                \n",
    "                log_probs = dist.log_prob(sampled_actions)\n",
    "                log_probs = torch.sum(log_probs, dim=1, keepdim=True)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # r(θ) =  π(a|s) / π_old(a|s)\n",
    "                ratio = (log_probs - sampled_log_probs_old).exp()\n",
    "                \n",
    "                # Surrogate Objctive : L_CPI(θ) = r(θ) * A\n",
    "                obj = ratio * sampled_advantages\n",
    "                \n",
    "                # clip ( r(θ), 1-Ɛ, 1+Ɛ )*A\n",
    "                obj_clipped = ratio.clamp(1.0 - PPO_CLIP_EPSILON, 1.0 + PPO_CLIP_EPSILON) * sampled_advantages\n",
    "                \n",
    "                # L_CLIP(θ) = E { min[ r(θ)A, clip ( r(θ), 1-Ɛ, 1+Ɛ )*A ] - β * KL }\n",
    "                policy_loss = -torch.min(obj, obj_clipped).mean(0) - BETA * entropy.mean()\n",
    "                \n",
    "                # L_VF(θ) = ( V(s) - V_t )^2\n",
    "                value_loss = 0.5 * (sampled_returns - values).pow(2).mean()\n",
    "               \n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                (policy_loss + value_loss).backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), GRADIENT_CLIP)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                batch_ind += BATCH_SIZE\n",
    "                batch_count += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function. NOT INTEGRATED YET\n",
    "\n",
    "def ppo_loss(model, states, actions, log_probs_old, returns, advantages):\n",
    "    dist, values = model(states)\n",
    "    \n",
    "    log_probs = dist.log_prob(actions)\n",
    "    log_probs = torch.sum(log_probs, dim=1, keepdim=True)\n",
    "    entropy = dist.entropy().mean()\n",
    "    \n",
    "    # r(θ) =  π(a|s) / π_old(a|s)\n",
    "    ratio = (log_probs - log_probs_old).exp()\n",
    "    \n",
    "    # Surrogate Objctive : L_CPI(θ) = r(θ) * A\n",
    "    obj = ratio * advantages\n",
    "    \n",
    "    # clip ( r(θ), 1-Ɛ, 1+Ɛ )*A\n",
    "    obj_clipped = ratio.clamp(1.0 - PPO_CLIP_EPSILON, 1.0 + PPO_CLIP_EPSILON) * advantages\n",
    "    \n",
    "    # L_CLIP(θ) = E { min[ r(θ)A, clip ( r(θ), 1-Ɛ, 1+Ɛ )*A ] - β * KL }\n",
    "    policy_loss = -torch.min(obj, obj_clipped).mean(0) - BETA * entropy.mean()\n",
    "    \n",
    "    # L_VF(θ) = ( V(s) - V_t )^2\n",
    "    value_loss = 0.5 * (returns - values).pow(2).mean()\n",
    "    \n",
    "    return policy_loss + value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the agent\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "\n",
    "def test_agent(env, agent, brain_name):\n",
    "    env_info = env.reset(train_mode = True)[brain_name]\n",
    "    num_agents = len(env_info.agents)\n",
    "    states = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    while True:\n",
    "        actions, _, _= agent.act(states)\n",
    "        env_info = env.step(actions.cpu().detach().numpy())[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def collect_trajectories(env, brain_name, agent, max_t):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    num_agents = len(env_info.agents)\n",
    "    states = env_info.vector_observations\n",
    "    # debug(states, \"states\")\n",
    "        \n",
    "    rollout = []\n",
    "    agents_rewards = np.zeros(num_agents)\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Collecting trajectories\n",
    "    for _ in range(max_t):\n",
    "    # for _ in range(5):\n",
    "        actions, log_probs, values = agent.act(states)\n",
    "        env_info = env.step(actions.cpu().detach().numpy())[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        # debug(next_states, \"next_states\")\n",
    "        rewards = env_info.rewards \n",
    "        dones = np.array([1 if t else 0 for t in env_info.local_done])\n",
    "        agents_rewards += rewards\n",
    "\n",
    "        for j, done in enumerate(dones):\n",
    "            if dones[j]:\n",
    "                episode_rewards.append(agents_rewards[j])\n",
    "                agents_rewards[j] = 0\n",
    "\n",
    "        rollout.append([states, values.detach(), actions.detach(), log_probs.detach(), rewards, 1 - dones])\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "    pending_value = agent.model(states)[-1]\n",
    "    returns = pending_value.detach() # Why is this called retuns? It's the value of the last state\n",
    "    rollout.append([states, pending_value, None, None, None, None])\n",
    "    \n",
    "    return rollout, returns, episode_rewards, np.mean(episode_rewards)\n",
    "\n",
    "\n",
    "def calculate_advantages(rollout, returns, num_agents):\n",
    "    \"\"\" Given a rollout, calculates the advantages for each state\n",
    "    \"\"\"\n",
    "    processed_rollout = [None] * (len(rollout) - 1)\n",
    "    advantages = torch.Tensor(np.zeros((num_agents, 1))) # advantages is a single value for each state\n",
    "\n",
    "    for i in reversed(range(len(rollout) - 1)):\n",
    "        states, value, actions, log_probs, rewards, dones = rollout[i]\n",
    "        dones = torch.Tensor(dones).unsqueeze(1)\n",
    "        rewards = torch.Tensor(rewards).unsqueeze(1)\n",
    "        actions = torch.Tensor(actions)\n",
    "        states = torch.Tensor(states)\n",
    "        next_value = rollout[i + 1][1]\n",
    "        \n",
    "        # V(s) = r + γ * V(s')\n",
    "        returns = rewards + GAMMA * dones * returns\n",
    "        \n",
    "        # L = r + γ*V(s') - V(s)\n",
    "        td_error = (rewards + GAMMA * dones * next_value.detach()) - value.detach() # targer - current\n",
    "        \n",
    "        advantages = advantages * TAU * GAMMA * dones + td_error\n",
    "        processed_rollout[i] = [states, actions, log_probs, returns, advantages]\n",
    "    \n",
    "    states, actions, log_probs_old, returns, advantages = map(lambda x: torch.cat(x, dim=0), zip(*processed_rollout))\n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "    \n",
    "    return states, actions, log_probs_old, returns, advantages\n",
    "\n",
    "\n",
    "def train(env, brain_name, agent, num_agents, n_episodes, max_t, run_name=\"testing_01\"):\n",
    "    print(f\"Starting training...\")\n",
    "    env.info = env.reset(train_mode = True)[brain_name]\n",
    "    all_scores = []\n",
    "    all_scores_window = deque(maxlen=100)\n",
    "    best_so_far = 1.0\n",
    "        \n",
    "    for i_episode in range(n_episodes):\n",
    "        # Each iteration, N parallel actors collect T time steps of data\n",
    "        rollout, returns, _, _ = collect_trajectories(env, brain_name, agent, max_t)\n",
    "        \n",
    "        states, actions, log_probs_old, returns, advantages = calculate_advantages(rollout, returns, num_agents)\n",
    "        # print(f\"States: {states.shape}. Actions: {actions.shape}. Log_probs_old: {log_probs_old.shape}. Returns: {returns.shape}. Advantages: {advantages.shape}\")\n",
    "        agent.learn(states, actions, log_probs_old, returns, advantages)\n",
    "        \n",
    "        test_mean_reward = test_agent(env, agent, brain_name)\n",
    "\n",
    "        all_scores.append(test_mean_reward)\n",
    "        all_scores_window.append(test_mean_reward)\n",
    "\n",
    "        if np.mean(all_scores_window) > best_so_far:\n",
    "            torch.save(agent.model.state_dict(), f\"ppo_checkpoint_{np.mean(all_scores_window)}.pth\")\n",
    "            best_so_far = np.mean(all_scores_window)\n",
    "            if np.mean(all_scores_window) > 30:\n",
    "                \n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(all_scores_window)))\n",
    "                # break       \n",
    "        \n",
    "        print('Episode {}, Total score this episode: {}, Last {} average: {}'.format(i_episode + 1, test_mean_reward, min(i_episode + 1, 100), np.mean(all_scores_window)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Load the environment\n",
    "from unityagents import UnityEnvironment\n",
    "import time\n",
    "# env = UnityEnvironment(file_name='../../unity_ml_envs/Reacher_Windows_x86_64/Reacher.exe')\n",
    "env = UnityEnvironment(file_name='../../PPO-Reacher_UnityML/Reacher_Windows_x86_64/Reacher.exe')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "time.sleep(2)\n",
    "\n",
    "# Environment variables\n",
    "num_agents = len(env_info.agents)\n",
    "state_size = env_info.vector_observations.shape[1]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# Training Hyperparameterso\n",
    "EPISODES = 200\n",
    "# MAX_T = 2048\n",
    "MAX_T = 1000\n",
    "SGD_EPOCHS = 4\n",
    "# optimizer parameters\n",
    "LR = 3e-4\n",
    "EPSILON = 1e-5\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TAU = 0.95              # GAE parameter\n",
    " \n",
    "# Instantiate the agent\n",
    "agent = Agent(num_agents, state_size, action_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "c:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total score this episode: 0.243999994546175, Last 1 average: 0.243999994546175\n",
      "Episode 2, Total score this episode: 0.18749999580904841, Last 2 average: 0.2157499951776117\n",
      "Episode 3, Total score this episode: 0.2924999934621155, Last 3 average: 0.241333327939113\n",
      "Episode 4, Total score this episode: 0.6644999851472676, Last 4 average: 0.3471249922411517\n",
      "Episode 5, Total score this episode: 0.5759999871253967, Last 5 average: 0.3928999912180007\n",
      "Episode 6, Total score this episode: 0.7054999842308461, Last 6 average: 0.4449999900534749\n",
      "Episode 7, Total score this episode: 0.9089999796822668, Last 7 average: 0.511285702857588\n",
      "Episode 8, Total score this episode: 1.2019999731332063, Last 8 average: 0.5976249866420403\n",
      "Episode 9, Total score this episode: 1.0934999755583703, Last 9 average: 0.6527222076327437\n",
      "Episode 10, Total score this episode: 1.3989999687299133, Last 10 average: 0.7273499837424606\n",
      "Episode 11, Total score this episode: 1.367499969433993, Last 11 average: 0.7855454369871453\n",
      "Episode 12, Total score this episode: 1.412999968416989, Last 12 average: 0.837833314606299\n",
      "Episode 13, Total score this episode: 1.7429999610409141, Last 13 average: 0.9074615181781923\n",
      "Episode 14, Total score this episode: 2.2914999487809835, Last 14 average: 1.0063214060783918\n",
      "Episode 15, Total score this episode: 2.3409999476745726, Last 15 average: 1.0952999755181372\n",
      "Episode 16, Total score this episode: 2.214999950490892, Last 16 average: 1.1652812239539343\n",
      "Episode 17, Total score this episode: 2.585999942198396, Last 17 average: 1.248852913262432\n",
      "Episode 18, Total score this episode: 3.0209999324753882, Last 18 average: 1.3473055254409294\n",
      "Episode 19, Total score this episode: 2.8059999372810127, Last 19 average: 1.424078915537776\n",
      "Episode 20, Total score this episode: 3.5109999215230348, Last 20 average: 1.5284249658370388\n",
      "Episode 21, Total score this episode: 3.3134999259375038, Last 21 average: 1.6134285353656326\n",
      "Episode 22, Total score this episode: 3.2674999269656837, Last 22 average: 1.6886135986201802\n",
      "Episode 23, Total score this episode: 3.864999913610518, Last 23 average: 1.783239090576282\n",
      "Episode 24, Total score this episode: 4.280999904312194, Last 24 average: 1.8873124578152785\n",
      "Episode 25, Total score this episode: 4.6789998954162, Last 25 average: 1.998979955319315\n",
      "Episode 26, Total score this episode: 4.82799989208579, Last 26 average: 2.107788414425718\n",
      "Episode 27, Total score this episode: 5.699999872595072, Last 27 average: 2.2408332832468054\n",
      "Episode 28, Total score this episode: 5.973999866470694, Last 28 average: 2.374160661219087\n",
      "Episode 29, Total score this episode: 5.753499871399254, Last 29 average: 2.4906895995011618\n",
      "Episode 30, Total score this episode: 5.759999871253967, Last 30 average: 2.5996666085595885\n",
      "Episode 31, Total score this episode: 6.638499851617962, Last 31 average: 2.7299515518840525\n",
      "Episode 32, Total score this episode: 6.875999846309424, Last 32 average: 2.8595155610848453\n",
      "Episode 33, Total score this episode: 7.810999825410545, Last 33 average: 3.0095605387916846\n",
      "Episode 34, Total score this episode: 6.720499849785119, Last 34 average: 3.1187058126444325\n",
      "Episode 35, Total score this episode: 6.965999844297767, Last 35 average: 3.2286284992630994\n",
      "Episode 36, Total score this episode: 7.798499825689942, Last 36 average: 3.355569369441623\n",
      "Episode 37, Total score this episode: 8.383999812602998, Last 37 average: 3.4914728949324707\n",
      "Episode 38, Total score this episode: 7.29449983695522, Last 38 average: 3.5915525513014908\n",
      "Episode 39, Total score this episode: 9.865999779477715, Last 39 average: 3.752435813562419\n",
      "Episode 40, Total score this episode: 9.531499786954374, Last 40 average: 3.896912412897218\n",
      "Episode 41, Total score this episode: 10.383499767910688, Last 41 average: 4.055121860580473\n",
      "Episode 42, Total score this episode: 10.683499761205166, Last 42 average: 4.212940382023918\n",
      "Episode 43, Total score this episode: 11.480999743379652, Last 43 average: 4.381965018334516\n",
      "Episode 44, Total score this episode: 13.191499705146999, Last 44 average: 4.582181715762073\n",
      "Episode 45, Total score this episode: 12.460999721474945, Last 45 average: 4.75726656033347\n",
      "Episode 46, Total score this episode: 12.993999709561468, Last 46 average: 4.936325976621036\n",
      "Episode 47, Total score this episode: 12.150999728403985, Last 47 average: 5.089829673467481\n",
      "Episode 48, Total score this episode: 13.627499695401639, Last 48 average: 5.267697798924442\n",
      "Episode 49, Total score this episode: 15.03599966391921, Last 49 average: 5.46705089821005\n",
      "Episode 50, Total score this episode: 15.094999662600458, Last 50 average: 5.659609873497858\n",
      "Episode 51, Total score this episode: 16.249999636784196, Last 51 average: 5.867264574738766\n",
      "Episode 52, Total score this episode: 16.814499624166636, Last 52 average: 6.077788325689302\n",
      "Episode 53, Total score this episode: 17.021499619539828, Last 53 average: 6.284273444441199\n",
      "Episode 54, Total score this episode: 17.609499606397, Last 54 average: 6.493999854847788\n",
      "Episode 55, Total score this episode: 18.95799957625568, Last 55 average: 6.720618031600659\n",
      "Episode 56, Total score this episode: 20.30549954613671, Last 56 average: 6.963205201503088\n",
      "Episode 57, Total score this episode: 18.955999576300382, Last 57 average: 7.173605102815322\n",
      "Episode 58, Total score this episode: 18.326499590370805, Last 58 average: 7.36589638708352\n",
      "Episode 59, Total score this episode: 22.42249949881807, Last 59 average: 7.621093049994275\n",
      "Episode 60, Total score this episode: 20.282499546650797, Last 60 average: 7.832116491605217\n",
      "Episode 61, Total score this episode: 20.3239995457232, Last 61 average: 8.036901459705511\n",
      "Episode 62, Total score this episode: 18.929499576892702, Last 62 average: 8.212588526111757\n",
      "Episode 63, Total score this episode: 21.86699951123446, Last 63 average: 8.429325208415293\n",
      "Episode 64, Total score this episode: 20.140999549813568, Last 64 average: 8.61232011999964\n",
      "Episode 65, Total score this episode: 23.785499468352647, Last 65 average: 8.845753648435842\n",
      "Episode 66, Total score this episode: 21.475999519973993, Last 66 average: 9.037121010125814\n",
      "Episode 67, Total score this episode: 20.926499532256276, Last 67 average: 9.21457442090388\n",
      "Episode 68, Total score this episode: 21.938499509636312, Last 68 average: 9.401690966326417\n",
      "Episode 69, Total score this episode: 22.452999498136343, Last 69 average: 9.590840365338154\n",
      "Episode 70, Total score this episode: 24.23199945837259, Last 70 average: 9.799999780952932\n",
      "Episode 71, Total score this episode: 25.18349943710491, Last 71 average: 10.01666879019451\n",
      "Episode 72, Total score this episode: 26.569499406125395, Last 72 average: 10.246569215415771\n",
      "Episode 73, Total score this episode: 26.81349940067157, Last 73 average: 10.473513464528866\n",
      "Episode 74, Total score this episode: 25.389999432489276, Last 74 average: 10.675087599231034\n",
      "Episode 75, Total score this episode: 26.918999398313463, Last 75 average: 10.891673089885465\n",
      "Episode 76, Total score this episode: 26.376999410428105, Last 76 average: 11.095427383576816\n",
      "Episode 77, Total score this episode: 27.444999386556447, Last 77 average: 11.307759487511616\n",
      "Episode 78, Total score this episode: 24.89649944351986, Last 78 average: 11.481974102332234\n",
      "Episode 79, Total score this episode: 27.498999385349453, Last 79 average: 11.684721257813464\n",
      "Episode 80, Total score this episode: 27.109499394055455, Last 80 average: 11.877530984516488\n",
      "Episode 81, Total score this episode: 26.371999410539864, Last 81 average: 12.056475039158752\n",
      "Episode 82, Total score this episode: 27.929999375715852, Last 82 average: 12.250054604238716\n",
      "Episode 83, Total score this episode: 27.07249939488247, Last 83 average: 12.428638276415148\n",
      "Episode 84, Total score this episode: 28.6044993606396, Last 84 average: 12.621208051227345\n",
      "Episode 85, Total score this episode: 31.620999293215572, Last 85 average: 12.844735007015442\n",
      "Episode 86, Total score this episode: 30.47999931871891, Last 86 average: 13.049796219942227\n",
      "Episode 87, Total score this episode: 29.929499331023543, Last 87 average: 13.243815795931669\n",
      "Episode 88, Total score this episode: 30.832999310828747, Last 88 average: 13.443692881328223\n",
      "Episode 89, Total score this episode: 30.465999319031834, Last 89 average: 13.634954751414782\n",
      "Episode 90, Total score this episode: 30.443999319523574, Last 90 average: 13.821721913282657\n",
      "Episode 91, Total score this episode: 31.7044992913492, Last 91 average: 14.018235950404268\n",
      "Episode 92, Total score this episode: 33.82299924399704, Last 92 average: 14.23350511663897\n",
      "Episode 93, Total score this episode: 34.37299923170358, Last 93 average: 14.450058816800954\n",
      "Episode 94, Total score this episode: 34.30649923318997, Last 94 average: 14.66129754463488\n",
      "Episode 95, Total score this episode: 35.653999203070995, Last 95 average: 14.882273351565786\n",
      "Episode 96, Total score this episode: 35.146499214414504, Last 96 average: 15.093359037637128\n",
      "Episode 97, Total score this episode: 34.142499236855656, Last 97 average: 15.289741926288865\n",
      "Episode 98, Total score this episode: 36.156499191839245, Last 98 average: 15.502668020835298\n",
      "Episode 99, Total score this episode: 35.689999202266335, Last 99 average: 15.70658045701137\n",
      "Episode 100, Total score this episode: 36.515499183814974, Last 100 average: 15.914669644279405\n",
      "Episode 101, Total score this episode: 37.3904991642572, Last 100 average: 16.286134635976513\n",
      "Episode 102, Total score this episode: 37.76849915580824, Last 100 average: 16.661944627576506\n",
      "Episode 103, Total score this episode: 37.77299915570766, Last 100 average: 17.03674961919896\n",
      "Episode 104, Total score this episode: 38.535499138664456, Last 100 average: 17.415459610734132\n",
      "Episode 105, Total score this episode: 38.306999143771826, Last 100 average: 17.7927696023006\n",
      "Episode 106, Total score this episode: 38.88599913083017, Last 100 average: 18.17457459376659\n",
      "Episode 107, Total score this episode: 38.50899913925677, Last 100 average: 18.550574585362337\n",
      "Episode 108, Total score this episode: 38.42699914108962, Last 100 average: 18.9228245770419\n",
      "Episode 109, Total score this episode: 38.46899914015084, Last 100 average: 19.29657956868783\n",
      "Episode 110, Total score this episode: 38.96249912912026, Last 100 average: 19.672214560291728\n",
      "Episode 111, Total score this episode: 38.86949913119897, Last 100 average: 20.04723455190938\n",
      "Episode 112, Total score this episode: 38.87849913099781, Last 100 average: 20.42188954353519\n",
      "Episode 113, Total score this episode: 38.745999133959415, Last 100 average: 20.791919535264373\n",
      "Episode 114, Total score this episode: 38.95599912926555, Last 100 average: 21.158564527069217\n",
      "Episode 115, Total score this episode: 39.037999127432705, Last 100 average: 21.5255345188668\n",
      "Episode 116, Total score this episode: 38.8284991321154, Last 100 average: 21.891669510683037\n",
      "Episode 117, Total score this episode: 38.66199913583696, Last 100 average: 22.252429502619425\n",
      "Episode 118, Total score this episode: 39.098499126080426, Last 100 average: 22.613204494555475\n",
      "Episode 119, Total score this episode: 38.97649912880733, Last 100 average: 22.974909486470743\n",
      "Episode 120, Total score this episode: 38.863499131333086, Last 100 average: 23.32843447856884\n",
      "Episode 121, Total score this episode: 38.789999132975936, Last 100 average: 23.683199470639227\n",
      "Episode 122, Total score this episode: 38.87699913103133, Last 100 average: 24.039294462679887\n",
      "Episode 123, Total score this episode: 39.25999912247062, Last 100 average: 24.39324445476849\n",
      "Episode 124, Total score this episode: 39.06049912692979, Last 100 average: 24.741039446994662\n",
      "Episode 125, Total score this episode: 38.88749913079664, Last 100 average: 25.083124439348467\n",
      "Episode 126, Total score this episode: 39.16399912461638, Last 100 average: 25.42648443167377\n",
      "Episode 127, Total score this episode: 39.07649912657216, Last 100 average: 25.760249424213548\n",
      "Episode 128, Total score this episode: 38.98699912857264, Last 100 average: 26.090379416834566\n",
      "Episode 129, Total score this episode: 38.99749912833795, Last 100 average: 26.42281940940395\n",
      "Episode 130, Total score this episode: 39.11399912573397, Last 100 average: 26.756359401948753\n",
      "Episode 131, Total score this episode: 39.04299912732095, Last 100 average: 27.080404394705784\n",
      "Episode 132, Total score this episode: 39.12099912557751, Last 100 average: 27.40285438749846\n",
      "Episode 133, Total score this episode: 39.093999126181004, Last 100 average: 27.715684380506165\n",
      "Episode 134, Total score this episode: 38.9529991293326, Last 100 average: 28.038009373301637\n",
      "Episode 135, Total score this episode: 39.05149912713095, Last 100 average: 28.358864366129975\n",
      "Episode 136, Total score this episode: 39.172499124426395, Last 100 average: 28.67260435911734\n",
      "Episode 137, Total score this episode: 39.12249912554398, Last 100 average: 28.979989352246744\n",
      "Episode 138, Total score this episode: 39.08649912634864, Last 100 average: 29.29790934514069\n",
      "Episode 139, Total score this episode: 38.97799912877381, Last 100 average: 29.58902933863364\n",
      "Episode 140, Total score this episode: 39.37599911987782, Last 100 average: 29.887474331962878\n",
      "\n",
      "Environment solved in 140 episodes!\tAverage Score: 30.18\n",
      "Episode 141, Total score this episode: 39.18749912409112, Last 100 average: 30.175514325524677\n",
      "\n",
      "Environment solved in 141 episodes!\tAverage Score: 30.46\n",
      "Episode 142, Total score this episode: 39.172499124426395, Last 100 average: 30.460404319156893\n",
      "\n",
      "Environment solved in 142 episodes!\tAverage Score: 30.73\n",
      "Episode 143, Total score this episode: 38.93049912983552, Last 100 average: 30.734899313021447\n",
      "\n",
      "Environment solved in 143 episodes!\tAverage Score: 31.00\n",
      "Episode 144, Total score this episode: 39.398499119374904, Last 100 average: 30.99696930716373\n",
      "\n",
      "Environment solved in 144 episodes!\tAverage Score: 31.26\n",
      "Episode 145, Total score this episode: 38.98699912857264, Last 100 average: 31.262229301234708\n",
      "\n",
      "Environment solved in 145 episodes!\tAverage Score: 31.52\n",
      "Episode 146, Total score this episode: 39.14399912506342, Last 100 average: 31.523729295389725\n",
      "\n",
      "Environment solved in 146 episodes!\tAverage Score: 31.79\n",
      "Episode 147, Total score this episode: 38.95149912936613, Last 100 average: 31.79173428939935\n",
      "\n",
      "Environment solved in 147 episodes!\tAverage Score: 32.05\n",
      "Episode 148, Total score this episode: 39.07499912660569, Last 100 average: 32.04620928371139\n",
      "\n",
      "Environment solved in 148 episodes!\tAverage Score: 32.29\n",
      "Episode 149, Total score this episode: 39.022499127779156, Last 100 average: 32.28607427834999\n",
      "\n",
      "Environment solved in 149 episodes!\tAverage Score: 32.52\n",
      "Episode 150, Total score this episode: 38.81549913240597, Last 100 average: 32.52327927304804\n",
      "\n",
      "Environment solved in 150 episodes!\tAverage Score: 32.75\n",
      "Episode 151, Total score this episode: 39.04749912722036, Last 100 average: 32.751254267952405\n",
      "\n",
      "Environment solved in 151 episodes!\tAverage Score: 32.97\n",
      "Episode 152, Total score this episode: 39.00599912814796, Last 100 average: 32.97316926299222\n",
      "\n",
      "Environment solved in 152 episodes!\tAverage Score: 33.19\n",
      "Episode 153, Total score this episode: 39.06099912691862, Last 100 average: 33.193564258066004\n",
      "\n",
      "Environment solved in 153 episodes!\tAverage Score: 33.41\n",
      "Episode 154, Total score this episode: 38.944999129511416, Last 100 average: 33.40691925329715\n",
      "\n",
      "Environment solved in 154 episodes!\tAverage Score: 33.61\n",
      "Episode 155, Total score this episode: 38.82549913218245, Last 100 average: 33.60559424885641\n",
      "\n",
      "Environment solved in 155 episodes!\tAverage Score: 33.79\n",
      "Episode 156, Total score this episode: 39.03499912749976, Last 100 average: 33.79288924467004\n",
      "\n",
      "Environment solved in 156 episodes!\tAverage Score: 34.00\n",
      "Episode 157, Total score this episode: 39.18549912413582, Last 100 average: 33.995184240148404\n",
      "\n",
      "Environment solved in 157 episodes!\tAverage Score: 34.20\n",
      "Episode 158, Total score this episode: 38.6939991351217, Last 100 average: 34.19885923559591\n",
      "\n",
      "Environment solved in 158 episodes!\tAverage Score: 34.36\n",
      "Episode 159, Total score this episode: 38.94949912941083, Last 100 average: 34.36412923190184\n",
      "\n",
      "Environment solved in 159 episodes!\tAverage Score: 34.55\n",
      "Episode 160, Total score this episode: 39.19399912394583, Last 100 average: 34.553244227674796\n",
      "\n",
      "Environment solved in 160 episodes!\tAverage Score: 34.74\n",
      "Episode 161, Total score this episode: 39.29549912167713, Last 100 average: 34.74295922343433\n",
      "\n",
      "Environment solved in 161 episodes!\tAverage Score: 34.95\n",
      "Episode 162, Total score this episode: 39.3154991212301, Last 100 average: 34.94681921887771\n",
      "\n",
      "Environment solved in 162 episodes!\tAverage Score: 35.12\n",
      "Episode 163, Total score this episode: 39.04399912729859, Last 100 average: 35.11858921503835\n",
      "\n",
      "Environment solved in 163 episodes!\tAverage Score: 35.31\n",
      "Episode 164, Total score this episode: 39.10549912592396, Last 100 average: 35.30823421079945\n",
      "\n",
      "Environment solved in 164 episodes!\tAverage Score: 35.46\n",
      "Episode 165, Total score this episode: 38.77949913321063, Last 100 average: 35.458174207448025\n",
      "\n",
      "Environment solved in 165 episodes!\tAverage Score: 35.64\n",
      "Episode 166, Total score this episode: 39.21049912357703, Last 100 average: 35.635519203484066\n",
      "\n",
      "Environment solved in 166 episodes!\tAverage Score: 35.82\n",
      "Episode 167, Total score this episode: 39.147999124974014, Last 100 average: 35.817734199411234\n",
      "\n",
      "Environment solved in 167 episodes!\tAverage Score: 35.99\n",
      "Episode 168, Total score this episode: 38.957999129220845, Last 100 average: 35.98792919560709\n",
      "\n",
      "Environment solved in 168 episodes!\tAverage Score: 36.15\n",
      "Episode 169, Total score this episode: 39.01149912802502, Last 100 average: 36.15351419190596\n",
      "\n",
      "Environment solved in 169 episodes!\tAverage Score: 36.30\n",
      "Episode 170, Total score this episode: 39.225499123241754, Last 100 average: 36.30344918855466\n",
      "\n",
      "Environment solved in 170 episodes!\tAverage Score: 36.44\n",
      "Episode 171, Total score this episode: 39.209999123588204, Last 100 average: 36.443714185419495\n",
      "\n",
      "Environment solved in 171 episodes!\tAverage Score: 36.57\n",
      "Episode 172, Total score this episode: 38.964499129075556, Last 100 average: 36.567664182649\n",
      "\n",
      "Environment solved in 172 episodes!\tAverage Score: 36.69\n",
      "Episode 173, Total score this episode: 39.217499123420566, Last 100 average: 36.69170417987649\n",
      "\n",
      "Environment solved in 173 episodes!\tAverage Score: 36.83\n",
      "Episode 174, Total score this episode: 38.801999132707714, Last 100 average: 36.825824176878676\n",
      "\n",
      "Environment solved in 174 episodes!\tAverage Score: 36.95\n",
      "Episode 175, Total score this episode: 39.14399912506342, Last 100 average: 36.94807417414617\n",
      "\n",
      "Environment solved in 175 episodes!\tAverage Score: 37.08\n",
      "Episode 176, Total score this episode: 39.218999123387036, Last 100 average: 37.07649417127577\n",
      "\n",
      "Environment solved in 176 episodes!\tAverage Score: 37.19\n",
      "Episode 177, Total score this episode: 38.95449912929907, Last 100 average: 37.19158916870319\n",
      "\n",
      "Environment solved in 177 episodes!\tAverage Score: 37.34\n",
      "Episode 178, Total score this episode: 39.26149912243709, Last 100 average: 37.33523916549237\n",
      "\n",
      "Environment solved in 178 episodes!\tAverage Score: 37.45\n",
      "Episode 179, Total score this episode: 39.21949912337586, Last 100 average: 37.452444162872624\n",
      "\n",
      "Environment solved in 179 episodes!\tAverage Score: 37.57\n",
      "Episode 180, Total score this episode: 39.12299912553281, Last 100 average: 37.5725791601874\n",
      "\n",
      "Environment solved in 180 episodes!\tAverage Score: 37.70\n",
      "Episode 181, Total score this episode: 39.14449912505224, Last 100 average: 37.70030415733252\n",
      "\n",
      "Environment solved in 181 episodes!\tAverage Score: 37.81\n",
      "Episode 182, Total score this episode: 39.20649912366643, Last 100 average: 37.81306915481203\n",
      "\n",
      "Environment solved in 182 episodes!\tAverage Score: 37.93\n",
      "Episode 183, Total score this episode: 39.026499127689746, Last 100 average: 37.932609152140095\n",
      "\n",
      "Environment solved in 183 episodes!\tAverage Score: 38.04\n",
      "Episode 184, Total score this episode: 39.14099912513048, Last 100 average: 38.03797414978501\n",
      "\n",
      "Environment solved in 184 episodes!\tAverage Score: 38.12\n",
      "Episode 185, Total score this episode: 39.3819991197437, Last 100 average: 38.11558414805029\n",
      "\n",
      "Environment solved in 185 episodes!\tAverage Score: 38.20\n",
      "Episode 186, Total score this episode: 39.22899912316352, Last 100 average: 38.203074146094735\n",
      "\n",
      "Environment solved in 186 episodes!\tAverage Score: 38.29\n",
      "Episode 187, Total score this episode: 38.81899913232773, Last 100 average: 38.29196914410778\n",
      "\n",
      "Environment solved in 187 episodes!\tAverage Score: 38.38\n",
      "Episode 188, Total score this episode: 39.314499121252446, Last 100 average: 38.37678414221202\n",
      "\n",
      "Environment solved in 188 episodes!\tAverage Score: 38.46\n",
      "Episode 189, Total score this episode: 39.137499125208706, Last 100 average: 38.46349914027379\n",
      "\n",
      "Environment solved in 189 episodes!\tAverage Score: 38.55\n",
      "Episode 190, Total score this episode: 39.287499121855944, Last 100 average: 38.551934138297106\n",
      "\n",
      "Environment solved in 190 episodes!\tAverage Score: 38.63\n",
      "Episode 191, Total score this episode: 39.08399912640452, Last 100 average: 38.625729136647664\n",
      "\n",
      "Environment solved in 191 episodes!\tAverage Score: 38.68\n",
      "Episode 192, Total score this episode: 39.40949911912903, Last 100 average: 38.68159413539898\n",
      "\n",
      "Environment solved in 192 episodes!\tAverage Score: 38.73\n",
      "Episode 193, Total score this episode: 39.27399912215769, Last 100 average: 38.730604134303526\n",
      "\n",
      "Environment solved in 193 episodes!\tAverage Score: 38.78\n",
      "Episode 194, Total score this episode: 39.360999120213094, Last 100 average: 38.781149133173756\n",
      "\n",
      "Environment solved in 194 episodes!\tAverage Score: 38.81\n",
      "Episode 195, Total score this episode: 38.70049913497642, Last 100 average: 38.81161413249281\n",
      "\n",
      "Environment solved in 195 episodes!\tAverage Score: 38.85\n",
      "Episode 196, Total score this episode: 39.214999123476446, Last 100 average: 38.85229913158342\n",
      "\n",
      "Environment solved in 196 episodes!\tAverage Score: 38.90\n",
      "Episode 197, Total score this episode: 39.00349912820384, Last 100 average: 38.9009091304969\n",
      "\n",
      "Environment solved in 197 episodes!\tAverage Score: 38.93\n",
      "Episode 198, Total score this episode: 38.93299912977964, Last 100 average: 38.928674129876306\n",
      "\n",
      "Environment solved in 198 episodes!\tAverage Score: 38.96\n",
      "Episode 199, Total score this episode: 39.01449912795797, Last 100 average: 38.96191912913323\n",
      "\n",
      "Environment solved in 199 episodes!\tAverage Score: 38.98\n",
      "Episode 200, Total score this episode: 38.530499138776214, Last 100 average: 38.98206912868284\n",
      "\n",
      "Environment solved in 200 episodes!\tAverage Score: 39.00\n",
      "Episode 201, Total score this episode: 38.88099913094193, Last 100 average: 38.99697412834968\n",
      "\n",
      "Environment solved in 201 episodes!\tAverage Score: 39.01\n",
      "Episode 202, Total score this episode: 39.00599912814796, Last 100 average: 39.00934912807308\n",
      "\n",
      "Environment solved in 202 episodes!\tAverage Score: 39.02\n",
      "Episode 203, Total score this episode: 38.9094991303049, Last 100 average: 39.02071412781905\n",
      "\n",
      "Environment solved in 203 episodes!\tAverage Score: 39.03\n",
      "Episode 204, Total score this episode: 39.04849912719801, Last 100 average: 39.02584412770439\n",
      "\n",
      "Environment solved in 204 episodes!\tAverage Score: 39.03\n",
      "Episode 205, Total score this episode: 38.83549913195893, Last 100 average: 39.031129127586254\n",
      "\n",
      "Environment solved in 205 episodes!\tAverage Score: 39.03\n",
      "Episode 206, Total score this episode: 38.99099912848324, Last 100 average: 39.032179127562785\n",
      "\n",
      "Environment solved in 206 episodes!\tAverage Score: 39.04\n",
      "Episode 207, Total score this episode: 39.151999124884604, Last 100 average: 39.038609127419065\n",
      "\n",
      "Environment solved in 207 episodes!\tAverage Score: 39.05\n",
      "Episode 208, Total score this episode: 39.22049912335351, Last 100 average: 39.0465441272417\n",
      "\n",
      "Environment solved in 208 episodes!\tAverage Score: 39.05\n",
      "Episode 209, Total score this episode: 38.76999913342297, Last 100 average: 39.049554127174424\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b71739d3c1a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPISODES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_T\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-107a3231da1a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, brain_name, agent, num_agents, n_episodes, max_t, run_name)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_advantages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m# print(f\"States: {states.shape}. Actions: {actions.shape}. Log_probs_old: {log_probs_old.shape}. Returns: {returns.shape}. Advantages: {advantages.shape}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mtest_mean_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a0a071a33989>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, states, actions, log_probs_old, returns, advantages, sgd_epochs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGRADIENT_CLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "train(env, brain_name, agent, num_agents, EPISODES, MAX_T)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Environment solved in 0 episodes!\tAverage Score: 39.05\n",
      "Episode 1, Total score this episode: 39.050999127142134, Last 1 average: 39.050999127142134\n",
      "Episode 2, Total score this episode: 38.54099913854152, Last 2 average: 38.79599913284183\n",
      "Episode 3, Total score this episode: 39.19849912384525, Last 3 average: 38.930165796509634\n",
      "Episode 4, Total score this episode: 39.00149912824854, Last 4 average: 38.94799912944436\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b71739d3c1a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPISODES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_T\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-107a3231da1a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, brain_name, agent, num_agents, n_episodes, max_t, run_name)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;31m# Each iteration, N parallel actors collect T time steps of data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0mrollout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollect_trajectories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_advantages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-107a3231da1a>\u001b[0m in \u001b[0;36mcollect_trajectories\u001b[1;34m(env, brain_name, agent, max_t)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# for _ in range(5):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0menv_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a0a071a33989>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, states)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mIt\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mcollecting\u001b[0m \u001b[0mtrajectories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \"\"\"\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# pass the state trough the network and get a distribution over actions and the value of the state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sample an action from the distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# calculate the log probability of that action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a0a071a33989>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, states)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# Critic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Actor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a0a071a33989>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, states)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "train(env, brain_name, agent, num_agents, EPISODES, MAX_T)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
