{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control: Unity ML Reacher environment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Solving Unity-ML Reacher environment using the Proximal Policy Optimization (PPO) algorithm\n",
    "\n",
    "Proximal Policy Optimization (PPO) is a reinforcement learning algorithm designed to address the challenges of policy-based optimization. It operates by updating the policy in a manner that doesn't change it too drastically, using a clipped objective function to prevent large policy updates. This ensures stable and efficient learning by avoiding overly aggressive updates that can harm the agent's performance.\n",
    "Also, when calculating the optimizing function (objective), we'll use the advantage calculation, instead of using the expected future return for each state.\n",
    "\n",
    "There are a few considerations for this implementation of the PPO algorithm that should be highlighted:\n",
    "\n",
    "1. ThisPPO often operates within an Actor-Critic framework. The \"Actor\" is the policy being optimized and the \"Critic\" estimates the value function, V(s). The advantage is used to coordinate updates between these two components. The critic's value function estimates help in computing the advantage, which in turn is used to update the actor.\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Environment\n",
    "\n",
    "\n",
    "## Reacher Environment Description:\n",
    "\n",
    "In the Reacher environment, an agent controls a double-jointed arm. The primary objective of the agent is to move its hand (the end of the arm) to a target location and keep it there. The target can move over time, which presents a continuous tracking challenge for the agent. The agent receives a reward based on the distance between the hand and the target - the closer the hand is to the target, the higher the reward.\n",
    "\n",
    "### Multi-Agent Variant:\n",
    "\n",
    "The multi-agent version of the Reacher environment contains multiple double-jointed arms, with each arm controlled by an independent agent. All agents share the same environment, but each agent is trained individually to control its own arm. The presence of multiple agents can make the task more challenging because the agents must learn not just to reach their own individual targets but also to potentially coordinate and avoid colliding with other agents.\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Load the environment\n",
    "from unityagents import UnityEnvironment\n",
    "import time\n",
    "# env = UnityEnvironment(file_name='../../unity_ml_envs/Reacher_Windows_x86_64/Reacher.exe')\n",
    "env = UnityEnvironment(file_name='../PPO-Reacher_UnityML/Reacher_Windows_x86_64/Reacher.exe')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n",
      "Size of state: (33,)\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print(f\"Size of state: {states[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "EPISODES = 1000         # Number of episodes to train for\n",
    "# MAX_T = 2048          # Max length of trajectory\n",
    "MAX_T = 1000            # Max length of trajectory\n",
    "SGD_EPOCHS = 4          # Number of gradient descent steps per batch of experiences\n",
    "BATCH_SIZE = 32         # minibatch size\n",
    "BETA = 0.01             # entropy regularization parameter\n",
    "GRADIENT_CLIP = 5       # gradient clipping parameter\n",
    "\n",
    "# optimizer parameters\n",
    "LR = 3e-4               # learning rate\n",
    "EPSILON = 1e-5          # optimizer epsilon\n",
    "\n",
    "# PPO parameters\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TAU = 0.95              # GAE parameter\n",
    "PPO_CLIP_EPSILON = 0.2  # ppo clip parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Agent\n",
    "\n",
    "For this PPO solution, we are going to use an Actor-Critic architecture, in which the actor chooses the action directly, while the critic evaluates each state for calculating the TD error estimate in the advantage calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent and models\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "    \n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, value_size=1, hidden_size=64):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, value_size)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    \n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, value_size=1, hidden_size=64, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = Actor(state_size, action_size, hidden_size)\n",
    "        self.critic = Critic(state_size, value_size, hidden_size)\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_size)*std)\n",
    "        \n",
    "    def forward(self, states): # TODO: LEARN WHAT THE FUCK THIS DOES\n",
    "        obs = torch.FloatTensor(states)\n",
    "        \n",
    "        # Critic\n",
    "        values = self.critic(obs)\n",
    "        \n",
    "        # Actor\n",
    "        mu = self.actor(obs)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        \n",
    "        return dist, values\n",
    "    \n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self, num_agents, state_size, action_size):\n",
    "        self.num_agents = num_agents\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.model = ActorCritic(state_size, action_size, value_size=1)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LR, eps=EPSILON)\n",
    "        self.model.train()\n",
    "        \n",
    "    def act(self, states):\n",
    "        \"\"\"Remember: states are state vectors for each agent\n",
    "        It is used when collecting trajectories\n",
    "        \"\"\"\n",
    "        dist, values = self.model(states)           # pass the state trough the network and get a distribution over actions and the value of the state\n",
    "        actions = dist.sample()                     # sample an action from the distribution\n",
    "        log_probs = dist.log_prob(actions)          # calculate the log probability of that action\n",
    "        log_probs = log_probs.sum(-1).unsqueeze(-1) # sum the log probabilities of all actions taken (in case of multiple actions) and reshape to (batch_size, 1)\n",
    "        \n",
    "        return actions, log_probs, values\n",
    "\n",
    "    \n",
    "    def learn(self, states, actions, log_probs_old, returns, advantages, sgd_epochs=4):\n",
    "        \"\"\" Performs a learning step given a batch of experiences\n",
    "        \n",
    "        Remmeber: in the PPO algorithm, we perform SGD_episodes (usually 4) weights update steps per batch\n",
    "        using the proximal policy ratio clipped objective function\n",
    "        \"\"\"        \n",
    "\n",
    "        num_batches = states.size(0) // BATCH_SIZE\n",
    "        for i in range(sgd_epochs):\n",
    "            batch_count = 0\n",
    "            batch_ind = 0\n",
    "            for i in range(num_batches):\n",
    "                sampled_states = states[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "                sampled_actions = actions[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "                sampled_log_probs_old = log_probs_old[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "                sampled_returns = returns[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "                sampled_advantages = advantages[batch_ind:batch_ind+BATCH_SIZE, :]\n",
    "                \n",
    "                L = ppo_loss(self.model, sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                (L).backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), GRADIENT_CLIP)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                batch_ind += BATCH_SIZE\n",
    "                batch_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### 4. Loss function: PPO Clipped Surrogate Function with value loss (critic)\n",
    "\n",
    "**PPO ratio**: By taking the ratio between the last policy (the one used to colelct the trajectory) and the current one (updated in the last SGD epoch), we ensure that the policy doesn't change too drastically, preventing potential instabilities in learning.\n",
    "\n",
    "**Clipping:** PPO's clipping mechanism ensures that the policy doesn't get updated too much in one step, preventing disruptive updates and ensuring smoother learning.\n",
    "\n",
    "**Entropy Bonus:** Including the entropy bonus ensures sufficient exploration, which is essential for finding optimal or near-optimal policies, especially in environments with deceptive local optima.\n",
    "\n",
    "**Value Loss:** Using a critic to estimate the value function can reduce the variance in policy updates, leading to faster and more stable convergence. The value loss ensures that this critic provides accurate value estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function. NOT INTEGRATED YET\n",
    "def ppo_loss(model, states, actions, log_probs_old, returns, advantages):\n",
    "    dist, values = model(states)\n",
    "    \n",
    "    log_probs = dist.log_prob(actions)\n",
    "    log_probs = torch.sum(log_probs, dim=1, keepdim=True)\n",
    "    entropy = dist.entropy().mean()\n",
    "    \n",
    "    # r(θ) =  π(a|s) / π_old(a|s)\n",
    "    ratio = (log_probs - log_probs_old).exp()\n",
    "    \n",
    "    # Surrogate Objctive : L_CPI(θ) = r(θ) * A\n",
    "    obj = ratio * advantages\n",
    "    \n",
    "    # clip ( r(θ), 1-Ɛ, 1+Ɛ )*A\n",
    "    obj_clipped = ratio.clamp(1.0 - PPO_CLIP_EPSILON, 1.0 + PPO_CLIP_EPSILON) * advantages\n",
    "    \n",
    "    # L_CLIP(θ) = E { min[ r(θ)A, clip ( r(θ), 1-Ɛ, 1+Ɛ )*A ] - β * KL }\n",
    "    policy_loss = -torch.min(obj, obj_clipped).mean(0) - BETA * entropy.mean()\n",
    "    \n",
    "    # L_VF(θ) = ( V(s) - V_t )^2\n",
    "    value_loss = 0.5 * (returns - values).pow(2).mean()\n",
    "    \n",
    "    return policy_loss + value_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Advantage Calculation\n",
    "\n",
    "Why would we use the advantage instead of using the raw future rewards when calculating the objective function?\n",
    "\n",
    "**Variance Reduction:** Raw future rewards are an \"infinite n-step\" bootstrap, and use full rollouts when calculating, which means that is has high variance. This is because the rewards in many environments can be stochastic, leading to different return values even for the same state-action pairs.\n",
    "The advantage function, A(s,a)=Q(s,a)−V(s), subtracts the value of being in state s from the Q-value, resulting in a measure that tells us how much better an action a is than the average action in that state. By centering our updates around this advantage, we're essentially normalizing our updates, leading to more stable learning.\n",
    "\n",
    "**Bias-Variance Trade-off:** In reinforcement learning, there's a trade-off between bias and variance. Using raw future rewards can have low bias but high variance. On the other hand, using just the value function (like a TD error) might have lower variance but higher bias. The advantage aims to strike a balance by considering both the current estimate (value function) and the sampled return.\n",
    "\n",
    "**Better Signal for Learning:** The advantage provides a more direct signal about the quality of an action. If an action's advantage is positive, it means the action is better than what's typically expected in that state. If it's negative, it means the action is worse. This kind of differentiation might be lost when using raw future rewards, especially in states where all actions lead to similarly high rewards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(rollout, returns, num_agents):\n",
    "    \"\"\" Given a rollout, calculates the advantages for each state \"\"\"\n",
    "    num_steps = len(rollout) - 1\n",
    "    processed_rollout = [None] * num_steps\n",
    "    advantages = torch.zeros((num_agents, 1))\n",
    "\n",
    "    for i in reversed(range(num_steps)):\n",
    "        states, value, actions, log_probs, rewards, dones = map(lambda x: torch.Tensor(x), rollout[i])\n",
    "        next_value = rollout[i + 1][1]\n",
    "\n",
    "        dones = dones.unsqueeze(1)\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "\n",
    "        # Compute the updated returns\n",
    "        returns = rewards + GAMMA * dones * returns\n",
    "\n",
    "        # Compute temporal difference error\n",
    "        td_error = rewards + GAMMA * dones * next_value.detach() - value.detach()\n",
    "        \n",
    "        advantages = advantages * TAU * GAMMA * dones + td_error\n",
    "        processed_rollout[i] = [states, actions, log_probs, returns, advantages]\n",
    "\n",
    "    # Concatenate along the appropriate dimension\n",
    "    states, actions, log_probs_old, returns, advantages = map(lambda x: torch.cat(x, dim=0), zip(*processed_rollout))\n",
    "    \n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    return states, actions, log_probs_old, returns, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Collect trajectories and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "from src.utils import test_agent\n",
    "import os\n",
    "\n",
    "def collect_trajectories(env, brain_name, agent, max_t):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    num_agents = len(env_info.agents)\n",
    "    states = env_info.vector_observations\n",
    "        \n",
    "    rollout = []\n",
    "    agents_rewards = np.zeros(num_agents)\n",
    "    episode_rewards = []\n",
    "\n",
    "    for _ in range(max_t):\n",
    "        actions, log_probs, values = agent.act(states)\n",
    "        env_info = env.step(actions.cpu().detach().numpy())[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards \n",
    "        dones = np.array([1 if t else 0 for t in env_info.local_done])\n",
    "        agents_rewards += rewards\n",
    "\n",
    "        for j, done in enumerate(dones):\n",
    "            if dones[j]:\n",
    "                episode_rewards.append(agents_rewards[j])\n",
    "                agents_rewards[j] = 0\n",
    "\n",
    "        rollout.append([states, values.detach(), actions.detach(), log_probs.detach(), rewards, 1 - dones])\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "    pending_value = agent.model(states)[-1]\n",
    "    returns = pending_value.detach() \n",
    "    rollout.append([states, pending_value, None, None, None, None])\n",
    "    \n",
    "    return rollout, returns, episode_rewards, np.mean(episode_rewards)\n",
    "\n",
    "\n",
    "def train(env, brain_name, agent, num_agents, n_episodes, max_t, run_name=\"testing_01\"):\n",
    "    print(f\"Starting training...\")\n",
    "    env.info = env.reset(train_mode = True)[brain_name]\n",
    "    all_scores = []\n",
    "    all_scores_window = deque(maxlen=100)\n",
    "    best_so_far = 0.0\n",
    "        \n",
    "    for i_episode in range(n_episodes):\n",
    "        # Each iteration, N parallel actors collect T time steps of data\n",
    "        rollout, returns, _, _ = collect_trajectories(env, brain_name, agent, max_t)\n",
    "        \n",
    "        states, actions, log_probs_old, returns, advantages = calculate_advantages(rollout, returns, num_agents)\n",
    "        # print(f\"States: {states.shape}. Actions: {actions.shape}. Log_probs_old: {log_probs_old.shape}. Returns: {returns.shape}. Advantages: {advantages.shape}\")\n",
    "        agent.learn(states, actions, log_probs_old, returns, advantages)\n",
    "        \n",
    "        test_mean_reward = test_agent(env, agent, brain_name)\n",
    "\n",
    "        all_scores.append(test_mean_reward)\n",
    "        all_scores_window.append(test_mean_reward)\n",
    "\n",
    "        if np.mean(all_scores_window) > best_so_far:\n",
    "            if not os.path.isdir(f\"./ckpt/{run_name}/\"):\n",
    "                os.mkdir(f\"./ckpt/{run_name}/\")\n",
    "            torch.save(agent.model.state_dict(), f\"./ckpt/{run_name}/ppo_checkpoint_{np.mean(all_scores_window)}.ckpt\")\n",
    "            best_so_far = np.mean(all_scores_window)\n",
    "            if np.mean(all_scores_window) > 30:\n",
    "                \n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(all_scores_window)))\n",
    "                # break       \n",
    "        \n",
    "        print('Episode {}, Total score this episode: {}, Last {} average: {}'.format(i_episode + 1, test_mean_reward, min(i_episode + 1, 100), np.mean(all_scores_window)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "time.sleep(2)\n",
    "\n",
    "# Environment variables\n",
    "num_agents = len(env_info.agents)\n",
    "state_size = env_info.vector_observations.shape[1]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = Agent(num_agents, state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episode 1, Total score this episode: 0.3714999916963279, Last 1 average: 0.3714999916963279\n",
      "Episode 2, Total score this episode: 0.363999991863966, Last 2 average: 0.36774999178014695\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b71739d3c1a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPISODES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_T\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-2a78fa7f8335>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, brain_name, agent, num_agents, n_episodes, max_t, run_name)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_advantages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m# print(f\"States: {states.shape}. Actions: {actions.shape}. Log_probs_old: {log_probs_old.shape}. Returns: {returns.shape}. Advantages: {advantages.shape}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mtest_mean_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-e887d4d286da>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, states, actions, log_probs_old, returns, advantages, sgd_epochs)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[0msampled_advantages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madvantages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_ind\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_ind\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                 \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppo_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampled_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampled_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampled_log_probs_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampled_returns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampled_advantages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-b85a6e87511c>\u001b[0m in \u001b[0;36mppo_loss\u001b[1;34m(model, states, actions, log_probs_old, returns, advantages)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# L_CLIP(θ) = E { min[ r(θ)A, clip ( r(θ), 1-Ɛ, 1+Ɛ )*A ] - β * KL }\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mpolicy_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj_clipped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mBETA\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mentropy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# L_VF(θ) = ( V(s) - V_t )^2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "train(env, brain_name, agent, num_agents, EPISODES, MAX_T)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.60449913712218"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils import load_trained_agent\n",
    "agent = load_trained_agent(env, \"./ckpt/run_2/ppo_checkpoint_36.132574192374015.pth\")\n",
    "test_agent(env, agent, brain_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
